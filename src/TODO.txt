
3. Tokenizacja pliku:
- poprawic: usuwanie komentarzy - std::string pomoc = "Pojemnosc plecaka: " + std::to_string((_Longlong)(pojemnosc)) + "\n"; /* 
- poprawic: konsola - pomoc to_string((_Longlong)(przedmiot[i].numer + 1)) + "" + to_string((_Longlong)(przedmiot[i].rozmiar)) + to_string( (_Longlong)(przedmiot[i].wartosc)) + "";
- refaktoryzacja - czytelny kod, tak żeby wiadomo było co kązdy if dokładnie robi
- przy usuwaniu komentarzy przekazac buffer?

- spis tokenów:
    + class Foo - CLASS,
    + void funkcja(int a, String b) - FUNCTION_DEF
    + funkcja(), cout << "hejka" - FUNCTION_USE
    + int, long, double - NUMBER
    + bool - BOOL
    + liczba++ - ASSIGN
    + int i = 0 - NUMBER, ASSIGN
    + string, char - TEXT
    + int* wsk, int *wsk - POINTER
    + new, delete - POINTER_MEM
    + MyClass myClass = new MyClass() - VAR, ASSIGN
    + if, switch - CONDITION
    + for, while, do while - LOOP
    + try, catch - EXCEPTION
    + ignorowanie const
    + if, while, do while - zrobić tylko token, że wystąpiło (pominąć gdzie się zaczyna i kończy)
    + pusta linia lub samo { lub samo } - EMPTY
    + cout - COUT
    + cin - CIN
- sprawdzić instrukcje w c++ pod kątem czyszczenia kodu i tokenizacji
- przetestowac tokenizacje dla pojedynczych plikow od różnych osób (wyświetlić i sprawdzić poprawność tokenizacji)


4. Algorytm:
- opcje - wybór minimalnej długości łańcuchów
- algorytm porównujący:
    + zasada działania:
        - z pary plików otrzymujemy dwa gigantyczne ciągi tokenów
        - szukamy takich samych fragmentów tokenów pomiędzy obydwoma ciągami, np. Rabin–Karp algorithm - porównać mniejszy string z większym, Rabin fingerprint, Dynamic pattern matching and hashing to B buckets
        - przeszukujemy gigantyczne ciągi i osobno zliczamy liczbę podobnych linii kodu (puste linii lub zawierające tylko nawias klamrowy, nie zaliczać do podobnych linii)
    + bierze pod uwagę minimalną długość fragmentu
    + raz znalezione identyczne tokeny, nie mogą być użyte do dalszych porównań
    + bierze pod uwagę procentowe podobieństwo fragmentów (niepodobny kod na początku i końcu fragmentu jest pomijany)
    + zwraca dane (numery linii z podobnymi fragmentami kodów)
- (w przypadku zbyt dużej liczby fałszywych wyników):
    + sprecyzować tokeny: NUMBER - NUMBER_WHOLE, NUMBER_DECIMAL - INT, LONG, FLOAT, DOUBLE
    + sprawdzać argumenty w definiowanych i wywoływanych funkcjach i klasach - sprawdzać również kolejność argumentów
    + sprawdzać argumenty w warunkach, np. if (i < 10), while (x == 0)
- opcje(2):
    + wybór procentu podobieństwa łańcuchów
    + wyświetlanie tylko par plików z minimalnym procentem podobieństwa
- help - opis wszystkiego m.in. kolumn tabel


X. Dodatkowe pomysły:
- wyświetlenie paska postępu i przeminiętego czasu
- algorytm dodatkowe:
    + dla linii kodu wykrytych jako podobne (tokeny), sprawdzić czy ich kod jest taki sam (jeśli taki sam to bardziej prawdopodobne, że plagiat)
    + opcje:
       - dokładna tokenizacja: LICZBA rozbić na INT, LONG, SHORT
    + szybkie porównanie:
        - sprawdza tylko procentowe podobieństwo (suma różnic takich samych linii w obu plikach przez sume linii)
        - nie bierze pod uwagę minimalnej długości fragmentu kodu
- statystyki:
    + czas wykonania programu
    + liczba identycznych tokenów
    + inne
- nazwy/szablony plików lub ścieżki podane przez użytkownika, które nie będą sprawdzane
- usuniecie niebezpiecznych znaków przed tokenizacja (twarda spacja, nieznane znaki unicode)
- ustawić znak końca linii jako średnik?
